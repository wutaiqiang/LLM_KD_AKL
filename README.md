# LLM_KD_AKL

This is the offcial github for the paper [Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2404.02657).

Coming soon.
